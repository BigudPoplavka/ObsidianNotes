#Машинное_обучение 

## Обычный метод ближайших соседей

**К-ближайших соседей (K-Nearest Neighbors или просто KNN)** — алгоритм классификации и регрессии, основанный на гипотезе компактности, которая предполагает, что расположенные близко друг к другу объекты в пространстве признаков имеют схожие значения целевой переменной или принадлежат к одному классу.

Суть метода в поиске наиближайшего соседа.
### Метрики расстояний

#### L2 - Эвклидово расстояние

![[Pasted image 20240808160454.png]]
Эвклидово расстояние между двумя точками, вычисляемое как корень из суммы квадратов разностей их координат по измерениям. Представляет из себя расстояние по прямой между точками.
#### L1 - Расстояние

![[Pasted image 20240808160424.png]]
![[Pasted image 20240808160556.png]]
Рассчитывается, как сумма модулей разностей координат. - сумма разниц по координатам (по всем измерениям).
## K-ближайших соседей

Это улучшение метода ближ. соседей, состоящий в том, чтобы принимать решение о классификации на основании не одного ближайшего соседа, а несколько, определенное пространство вокруг интересующей нас точки.

Мы смотрим в области какого класса мы находимся и исключаем вероятность случая, когда мы определим неправильный класс из-за попавшейся  ближайшей точки другого класса.
### Выбор К

**K** - это гиперпараметр. (то есть мы не подбираем его при тренировке, а задаем извне)

**overfitting** - излишне сложная и гибкая модель. В таком случае граница между классами не генерализуется, а просто повторяет разделение как на тестовых данных
**underfitting** - не достаточно сложная модель для описания границы между классами, из-за чего плохо разделяет классы

![[Pasted image 20240809084421.png]]

При обучении стремимся к **appropriate-fitting**



