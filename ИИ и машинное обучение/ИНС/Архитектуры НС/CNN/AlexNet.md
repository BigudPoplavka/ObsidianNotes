#Машинное_обучение #Нейросети 

[[Сверточные нейросети]]
[[Дропаут]]
[[Стохастический градиентный спуск]]
[[Pooling]]

![[Pasted image 20240725131120.png]]

**AlexNet** — сверточная нейронная сеть, которая оказала большое влияние на развитие машинного обучения, в особенности — на алгоритмы компьютерного зрения. Сеть с большим отрывом выиграла конкурс по распознаванию изображений ImageNet LSVRC-2012 в 2012 году (с количеством ошибок 15,3% против 26,2% у второго места).

Архитектура AlexNet схожа с созданной Yann LeCum сетью **LeNet** Однако у AlexNet **больше фильтров на слое и вложенных сверточных слоев**. Сеть включает в себя свертки, максимальное объединение, дропаут, аугментацию данных, функции активаций ReLU и стохастический градиентный спуск.

## Особенности AlexNet

1. Как функция активации используется **Relu** вместо **арктангенса** для добавления в модель нелинейности. За счет этого при одинаковой точности метода скорость становится в 6 раз быстрее.
2. Использование дропаута вместо регуляризации решает проблему переобучения. Однако время обучения удваивается с показателем дропаута 0,5.
3. Производится перекрытие объединений для уменьшения размера сети. За счет этого уровень ошибок первого и пятого уровней снижаются до 0,4% и 0,3%, соответственно.

## Архитектура

![[Pasted image 20240725155710.png]]

 AlexNet содержит восемь слоев с весовыми коэффициентами.
 
>   - Первые пять из них сверточные, 
>   - остальные три — полносвязные. 
>   - Выходные данные пропускаются через функцию потерь **softmax**, которая формирует распределение 1000 меток классов. 
>   - Сеть максимизирует многолинейную логистическую регрессию, что эквивалентно максимизации среднего по всем обучающим случаям логарифма вероятности правильной маркировки по распределению ожидания.
>   - Ядра второго, четвертого и пятого сверточных слоев связаны только с теми картами ядра в предыдущем слое, которые находятся на одном и том же графическом процессоре.
>   - Ядра третьего сверточного слоя связаны со всеми картами ядер второго слоя. 
>   - Нейроны в полносвязных слоях связаны со всеми нейронами предыдущего слоя.

Рассмотрим формулы и параметры, используемые для вычисления размеров выходных данных после применения фильтров и операций пулинга.

==**Формула для вычисления размера выходного слоя**==

![[Pasted image 20240725161804.png]]
### Первый сверточный слой:

![[Pasted image 20240725161458.png]]
- Вход: **227x227x3**
- Фильтр: **11x11**
- Шаг (stride): **4**
- Количество фильтров: **96**

Выход = (277 - 11) / 4 + 1 = 55 
==Результат: **55x55x96**==
### Первый слой пулинга 

![[Pasted image 20240725162032.png]]
- Вход: **55x55x96**
- Фильтр: **3x3**
- Шаг (stride): **2**
- Операция: **Overlapping Max Pooling**

Выход = (55 - 3) / 2 + 1 = 27 
==Результат: **27x27x96**==
### Второй сверточный слой

![[Pasted image 20240725162353.png]]
- Вход: **27x27x96**
- Фильтр: **5x5**
- Шаг (stride): **1**
- Padding: **2**
- Количество фильтров: **256**

Выход ![[Pasted image 20240725162313.png]]

Выход = (27 - 5 + 2х2) / 1 + 1 = 27 
==Результат: **27x27x256**==
### Второй слой пулинга

![[Pasted image 20240725163048.png]]
- Вход: **27x27x256**
- Фильтр: **3x3**
- Шаг (stride): **2**
- Операция: **Overlapping Max Pooling**

Выход = (27 - 3) / 2 + 1 = 13 
==Результат: **13x13x256

### Третий сверточный слой

![[Pasted image 20240725163539.png]]
- Вход: **13x13x256**
- Фильтр: **3x3**
- Шаг (stride): **1**
- Padding: **1**
- Количество фильтров: **384**

Выход = (13 - 3 + 2х1) / 1 + 1 = 13 
==Результат: **13x13x384**==
### Четверный сверточный слой

![[Pasted image 20240725163845.png]]
- Вход: **13x13x384**
- Фильтр: **3x3**
- Шаг (stride): **1**
- Padding: **1**
- Количество фильтров: **384**

Выход = (13 - 3 + 2х1) / 1 + 1 = 13 
==Результат: **13x13x384**==
### Пятый сверточный слой

![[Pasted image 20240725163845.png]]
- Вход: **13x13x384**
- Фильтр: **3x3**
- Шаг (stride): **1**
- Padding: **1**
- Количество фильтров: **256**

Выход = (13 - 3 + 1х1) / 1 + 1 = 13 
==Результат: **13x13x256**==
### Третий слой пулинга

![[Pasted image 20240725164250.png]]
- Вход: **13x13x256**
- Фильтр: **3x3**
- Шаг (stride): **2**
- Операция: **Overlapping Max Pooling**

Выход = (13 - 3) / 2 + 1 = 6 
==Результат: **6x6x256**==
### Далее следуют полносвязные слои (Fully Connected, FC):

- Вход: **6x6x256 = 9216**
- Первый полносвязный слой: **4096 нейронов**
- Второй полносвязный слой: **4096 нейронов**
- Выходной слой: **1000 классов (Softmax)**

Таким образом, AlexNet содержит 5 сверточных слоев и 3 полносвязных слоя. Relu применяется после каждого сверточного и полносвязного слоя. Дропаут применяется перед первым и вторым полносвязными слоями. Сеть содержит 62,3 миллиона параметров и затрачивает 1,1 миллиарда вычислений при прямом проходе.  Сверточные слои, на которые приходится 6% всех параметров, производят 95% вычислений.
## Обучение

AlexNet проходит 90 эпох. Обучение занимает 6 дней одновременно на двух графических процессорах Nvidia Geforce GTX 580, что является причиной того, что сеть разделена на две части. Используется стохастический градиентный спуск со скоростью обучения 0,01, импульсом 0,9 и распадом весовых коэффициентов 0,0005. Скорость обучения делится на 10 после насыщения точности и снижается в 3 раза в течение обучения. Схема обновления весовых коэффициентов _w_ имеет вид:

![[Pasted image 20240725160352.png]]

где _i_ — номер итерации, _v_ — переменная импульса, а _epsilon_ — скорость обучения. В ходе всего этапа обучения скорость обучения выбиралась равной для всех слоев и корректировалась вручную. Последующая эвристика заключалась в том, чтобы разделить скорость обучения на 10, когда количество ошибок при проверке переставало уменьшаться.