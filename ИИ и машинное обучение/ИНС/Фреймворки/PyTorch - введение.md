#Машинное_обучение #Фреймворк

[[PyTorch - команды]] 

![[Pasted image 20240817161228.png]]
## Пример

![[Pasted image 20240817161438.png]]

### Находжение лосса и backpropagination

Для решения такого графа используем тензоры и автоградиент вызовом **backward** на лосс-функции для обратного прохода.

Данный блок кода до вызова `backward` - **==это прямой проход==**

![[Pasted image 20240817161709.png]]
### Реализация цикла оптимизации

![[Pasted image 20240817162117.png]]

Главным здесь является 
- `W.data.add(-0.1*W*grad.data)` - добавление к данным n значений от градиента `(-0.1)` в данном примере, .
- `W.grad.zero` - а затем для некст шага обнуляем градиент
### Блоки графа 

#### Граф

![[Pasted image 20240817163541.png]]
#### Код

![[Pasted image 20240817163513.png]]
### Создание модулей

![[Pasted image 20240817163842.png]]

