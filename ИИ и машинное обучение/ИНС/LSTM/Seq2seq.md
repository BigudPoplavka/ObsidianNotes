#Машинное_обучение #Нейросети #LSTM 

[[Attention]]
## Особенности


## Архитектура

Модель Sequence-to-Sequence (Seq2Seq) с LSTM включает в себя две RNN:

1. **Энкодер:** Обрабатывает входную последовательность и производит контекстное представление.
2. **Декодер:** Использует это представление для генерации выходной последовательности.

![[Pasted image 20240808105309.png]]

_Энкодер_ — принимает предложение на языке _A_ и сжимает его в вектор скрытого состояния.

_Декодер_ — выдает слово на языке _B_, принимает последнее скрытое состояние энкодера и предыдущее предсказанное слово.
### Рассмотрим пример работы _Seq2seq_ сети:

`x_i` — слова в предложении на языке _A_.
`h_i` — скрытое состояние энкодера.
_Блоки энкодера (зеленый)_ — блоки энкодера получающие на вход xi𝑥𝑖 и передающие скрытое состояние hiℎ𝑖 на следующую итерацию.
`d_i` — скрытое состояние декодера.
`y_i` — слова в предложении на языке _B_.

_Блоки декодера (фиолетовый)_ — блоки декодера получающие на вход yi−1𝑦𝑖−1 или специальный токен **start** в случае первой итерации и возвращаюшие yi𝑦𝑖 — слова в предложении на языке _B_. Передают di𝑑𝑖 — скрытое состояние декодера на следующую итерацию. Перевод считается завершенным при yi𝑦𝑖, равном специальному токену **end**.
### Пример использования обобщенного механизма внимания для задачи машинного перевода

Для лучшего понимания работы обобщенного механизма внимания будет рассмотрен пример его применения в задаче машинного перевода при помощи Seq2seq сетей для решения которой он изначально был представлен.

При добавлении механизма в данную архитектуру между  _Энкодером_ и _Декодером_ слоя механизма внимания получится следующая схема:

![[Pasted image 20240808110200.png]]

Здесь `x_i, h_i, d_i, y_i` имеют те же назначения, что и в варианте без механизма внимания.

_Агрегатор скрытых состояний энкодера (желтый)_ — агрегирует в себе все вектора `h_i` и возвращает всю последовательность векторов `h=[h1,h2,h3,h4]`
`c_i` — вектор контекста на итерации `i`

_Блоки механизма внимания (красный)_ — принимает hℎ и di−1𝑑𝑖−1, возвращает ci𝑐𝑖.

_Блоки декодера (фиолетовый)_ — по сравнению с обычной _Seq2seq_ сетью меняются входные данные. Теперь на итерации `i` на вход подается не `yi−1`, а конкатенация `yi−1` и `ci`.

Таким образом при помощи механизма внимания достигается "фокусирование" декодера на определенных скрытых состояниях. В случаях машинного перевода эта возможность помогает декодеру предсказывать на какие скрытые состояния при исходных определенных словах на языке _A_ необходимо обратить больше внимания при переводе данного слова на язык _B_. То есть на какие слова из исходного текста обратить внимание при переводе конкретного слова на язык назначения.
