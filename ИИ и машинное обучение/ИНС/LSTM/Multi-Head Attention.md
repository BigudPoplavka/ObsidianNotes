#LSTM #Обработка_последовательностей 

Главным элементом является **==multi-head attention==** 
Это специальный слой, позволяющий каждому входному вектору **представления с энкодера (hidden-state)** взаимодействовать с другими элементами последовательности (словами) через **Attention**. 

**Multi-head** означает, что таких блоков несколько. То есть таких attention-блоков параллельно тренируется несколько.

**==В RNN передавались HS, а в CNN - hidden-states==**

![[Pasted image 20240807144609.png]]

Слой принимает на вход 
- **вектор Query** - используется для определения важности элемента последовательности по отношению к другим
- **вектор Key** - ключи для сравнения по query
- **вектор Values** - значения соотв. элементов последовательности

1. Каждый из них проходит через **обучаемый линейный преобразователь** **linear**
2. Вычисляется скалярное произведение **Query** со всеми **K** поочередно
3. Прогонка dot-product-ов через softmax
4. Все полученные веса суммируются с **V**

Таких dot-ов и послед. взеш. сумм attention несколько, и все они потом конкатенируются и прогоняются через **обучаемый линейный преобразователь** перед выходом.

>  
> Если есть необходимость обращать внимание на несколько аспектов слов, то **==множество параллельных блоков attention==** дает сети возможность это сделать.  
> Такой трюк используется довольно часто — оказывается, что тупо разных начальных случайных весов хватает, чтобы толкать разные слои в разные стороны.  
>   
> _Что такое несколько аспектов слов?._  
> Например, у слова есть фичи про его смысловое значение и про грамматическое.  
> Хочется получить вектора, соответствующие соседям с точки зрения смысловой составляющей и с грамматической.