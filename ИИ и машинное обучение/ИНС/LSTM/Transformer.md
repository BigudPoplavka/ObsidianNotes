#Машинное_обучение #Нейросети #LSTM 

[[Attention]]
[[Multi-Head Attention]]
## Особенности

Новая архитектура называется Transformer, была разработана в Гугле. Эта сеть исключает из себя **CNN, RNN**, а фокусируется на использовании **attention**. 

Задача машинного перевода в deep learning сводится к работе с последовательностями (как и много других задач): мы тренируем модель, которая может получить на вход предложение как последовательность слов и выдать последовательность слов на другом языке. В текущих подходах внутри модели **обычно есть энкодер и декодер** — **==энкодер преобразует слова входного предложения в один или больше векторов в неком пространстве, декодер — генерирует из этих векторов последовательность слов на другом языке.==**  
## Архитектура

Стандартные архитектуры для энкодера — **RNN** или **CNN**, для декодера — чаще всего **RNN**. Но не в Transformer.

> ==**Трансформер**== - это последовательные друг за другом модели attention, позволяющие передавать последовательность с сохранением контекстной информации.  

![[Pasted image 20240807142154.png]]

## Encoder

**==Часть сети, принимающая на вход последовательность, и выдающая некие эмбеддинги, соответствующие каждому элементу последовательности, с которыми будет работать декодер.==** Элементы последовательности проходят параллельно через слои. Часть слоев FC,часть **shortcut** - (соединения, пропускающие один или несколько слоев). 

![[Pasted image 20240808104544.png]]
### Multi-Head Attention

Главным элементом является **==multi-head attention==** 
Это специальный слой, позволяющий каждому входному вектору **представления с энкодера (hidden-state)** взаимодействовать с другими элементами последовательности (словами) через **Attention**. 

**Multi-head** означает, что таких блоков несколько. То есть таких attention-блоков параллельно тренируется несколько.
### Принцип работы

1. На вход поступают элементы `w_i`
2. **Создаются эмбеддинги для всех элементов** (слов предложения) (вектора одинаковой размерности). Для примера пусть это будет предложение `I am great` В эмбеддинг добавляется еще позиция слова в предложении: 
	1. К каждому `w_i` добавляются позиционные векторы `p_i`
	2. Вектор `h_i = w_i + p_i`
3.  Полученный вектор `h_i` подается на **multi-head-attention** 
4. Берется вектор первого слова и вектор второго слова (`I`, `am`), подаются на однослойную сеть с одним выходом, **==которая выдает степень их похожести (скалярная величина).==** 
5. Эта скалярная величина умножается на вектор второго слова, получая его некоторую "ослабленную" на величину похожести копию.
6. Вместо второго слова подается третье слово и делается тоже самое что в п.2. с той же самой сетью с теми же весами (для векторов `I`, `great`).
7. Делая тоже самое для всех оставшихся слов предложения получаются их "ослабленные" (взвешенные) копии, которые выражают степень их похожести на первое слово. Далее эти все взвешенные вектора складываются друг с другом, получая один результирующий вектор размерности одного эмбединга:  
    `output = weight(I, am) * am + weight(I, great) * greqat`  
    **==Это механизм "обычного" attention.==**
    
5. Так как оценка похожести слов всего одним способом (по одному критерию) считается недостаточной, тоже самое (п.2-4) повторяется несколько раз с другими весами. То есть один attention может определять похожесть слов по смысловой нагрузке, другой по грамматической, остальные еще как-то и т.п.
6. На выходе п.5. получается несколько векторов, каждый из которых является взвешенной суммой всех остальных слов предложения относительно их похожести на первое слово (`I`). 
7. Конкантенируем этот вректор в один для получения той же размерности.
8. Дальше ставится еще один слой линейного преобразования, уменьшающий размерность результата п.6. до размерности вектора одного эмбединга. Получается некое представление первого слова предложения, составленное из взвешенных векторов всех остальных слов предложения.
9. Такой же процесс производится для всех других слов в предложении.
10. Так как размерность выхода та же, то можно проделать все тоже самое еще раз (п.2-8), но вместо оригинальных эмбеддингов слов взять то, что получается после прохода через этот Multi-head attention, а нейросети аттеншенов внутри взять с другими весами (веса между слоями не общие). И таких слоев можно сделать много (у гугла 6). Однако между первым и вторым слоем добавляется еще полносвязный слой и residual соединения, чтобы добавить сети выразительности.

## Decoder


![[Pasted image 20240808082820.png]]

На вход получает эмбеддинги с энкодера. 

Декодер тоже запускается по слову за раз, получает на вход прошлое слово и должен выдать следующее (на первой итерации получает специальный токен `<start>`).

**В декодере есть два разных типа использования Multi-head attention:**

- Первый — это возможность обратиться к векторам прошлых декодированных слов, также как и было в процессе encoding (но можно обращаться не ко всем, а только к уже декодированным).
- Второй — возможность обратиться к выходу энкодера. B этом случае Query — это вектор входа в декодере, а пары Key/Value — это финальные эмбеддинги энкодера, где опять же один и тот же вектор идет и как key, и как value (но линейные преобразования внутри attention module для них разные)  
В середине еще просто FC layer, опять те же residual connections и layer normalization.

И все это снова повторяется 6 раз, где выход предыдущего блока идет на вход следующему.

Наконец, в конце сети стоит обычный softmax, который выдает вероятности слов. Сэмплирование из него и есть результат, то есть следующее слово в предложении. Мы его даем на вход следующему запуску декодера и процесс повторяется, пока декодер не выдаст токен `<end of sentence>`.