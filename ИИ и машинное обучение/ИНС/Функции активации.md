#Машинное_обучение #Нейросети

[[Нейрон]]
[[Математическая функция]]
[[Сверточные нейросети]]
<h2>Функции активации</h2>

В искусственных нейронных сетях **функция активации** нейрона определяет выходной сигнал, который определяется входным сигналом или набором входных сигналов.

![[Pasted image 20220604164719.png]]

Функции активации играют ключевую роль в работе нейронных сетей, поскольку они определяют выход каждого нейрона и, таким образом, всей сети. Они добавляют нелинейность, что позволяет сети моделировать сложные данные и выполнять задачи, такие как классификация, регрессия и обнаружение объектов. Выбор функции активации зависит от конкретной задачи и архитектуры сети. Рассмотрим основные функции активации, их особенности и критерии выбора.

## Основные функции активации

1. **ReLU (Rectified Linear Unit)**
    - **Преимущества**:
        - Простота и эффективность вычислений.
        - Помогает избежать проблемы исчезающего градиента.
    - **Недостатки**:
        - Возможна проблема "умирающих" нейронов, когда выходные значения становятся постоянно равными нулю.
    - **Применение**: Широко используется в скрытых слоях сверточных и глубоких нейронных сетей.

1. **Sigmoid**
    - **Преимущества**:
        - Значения выходов ограничены (0, 1), что удобно для вероятностных интерпретаций.
    - **Недостатки**:
        - [[Проблема исчезающего градиента]].
        - Выходы не центрированы вокруг нуля, что может замедлить обучение.
    - **Применение**: Используется в выходных слоях для задач бинарной классификации.

1. **Tanh (Hyperbolic Tangent)**
    
    - **Формула**: f(x)=tanh⁡(x)
    - **Преимущества**:
        - Выходы ограничены (-1, 1) и центрированы вокруг нуля, что может улучшить скорость обучения.
    - **Недостатки**:
        - Проблема исчезающего градиента.
    - **Применение**: Используется в скрытых слоях, особенно когда требуется, чтобы выходы были центрированы.
    
1. **Softmax**
    
    - **Формула**: f(xi)=exi∑jexjf(x_i) 
    - **Преимущества**:
        - Преобразует выходы в вероятности, суммирующиеся до 1.
    - **Недостатки**:
        - Чувствительна к большим числам, что может требовать числовой стабильности.
    - **Применение**: Используется в выходных слоях для задач многоклассовой классификации.

### Критерии выбора функции активации

1. **Тип задачи**:
    - **Бинарная классификация**: Sigmoid в выходном слое.
    - **Многоклассовая классификация**: Softmax в выходном слое.
    - **Регрессия**: Линейная функция активации в выходном слое (без активации).

2. **Проблемы с градиентом**:
    - **Исчезающий градиент**: Выбирать функции, которые помогают избежать этой проблемы, такие как ReLU или Leaky ReLU.
    - **Взрывающийся градиент**: Использовать методы нормализации, такие как Batch Normalization, вместе с подходящей функцией активации.

3. **Центрирование выходов**:
    - **Выходы центрированы вокруг нуля**: Tanh предпочтительнее Sigmoid, так как это улучшает скорость обучения за счет центрированных выходов.

4. **Производительность и вычислительная эффективность**:
    - **Простота и скорость вычислений**: ReLU очень эффективен и широко используется из-за своей простоты.
    - **Использование в глубокой сети**: ReLU и его варианты (Leaky ReLU) лучше подходят для глубоких сетей.

5. **Специфические архитектуры**:
    - **Сверточные нейронные сети (CNN)**: ReLU или Leaky ReLU в скрытых слоях.
    - **Рекуррентные нейронные сети (RNN)**: Tanh или Sigmoid в рекуррентных компонентах, но современные RNN часто используют также ReLU.

### Поведение функций активации

1. **ReLU**: Хорош для моделирования линейных зависимостей с некоторыми нелинейностями. Быстро обучается, но может привести к "умирающим" нейронам.
2. **Sigmoid**: Подходит для задач, требующих вероятностных выходов, но склонен к исчезающему градиенту и медленному обучению.
3. **Tanh**: Центрирован вокруг нуля, что улучшает скорость обучения по сравнению с Sigmoid, но все еще подвержен исчезающему градиенту.
4. **Leaky ReLU**: Обеспечивает небольшие градиенты даже для отрицательных входов, что помогает избежать проблемы "умирающих" нейронов.
5. **Softmax**: Преобразует выходы в вероятности, что делает её идеальной для многоклассовых задач, но требует внимательного обращения с числовой стабильностью.

<h2>Сигмоида</h2>

Функция и ее производная
![[Pasted image 20220604183825.png]]

График
![[Pasted image 20220604155142.png]]

## Рассмотрим матричный пример:

![[Pasted image 20220604183421.png]]
<ul>
	<li><b>a</b> - это конкретный нейрон</li>
	<li><b>(0)</b> - верхний индекс - номер слоя</li>
	<li><b>1..n (i)</b> - это  индекс нейрона</li>
</ul>

1) Объединим все активации слоя (0) в столбец-вектор
2) Объединим все веса <b>w</b> в матрицу, где каждая строка - слой, соединяющийся со следующим нейроном следующего слоя
3) Вынесем прибавление сдвигов <b>b</b>

![[Pasted image 20220604184131.png]]

4) Применим к этому ф-цию активации (сигмоиду)
 ![[Pasted image 20220604184404.png]]

<h2>Пример матричного умножения на Python </h2>

![[Pasted image 20220604184556.png]]

## ReLU

![[Pasted image 20240819083808.png]]

