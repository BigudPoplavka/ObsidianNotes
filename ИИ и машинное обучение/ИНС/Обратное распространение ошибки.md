#Машинное_обучение #Нейросети 

[[Проблема исчезающего градиента]]
## Обратное распространение (Backpropagation) и обновление весов: Объяснение на примере

Давайте рассмотрим простую нейронную сеть и разберем процесс обратного распространения ошибки и обновления весов на примере.
## Пример нейронной сети

Предположим, у нас есть трехслойная нейронная сеть:

- **Входной слой**: 2 нейрона (для двух входных признаков *x1*​ и *x2*​)
- **Скрытый слой**: 2 нейрона
- **Выходной слой**: 1 нейрон (для предсказания значения *y*)

Для простоты будем использовать сигмоидную функцию активации.
![[Pasted image 20240724231722.png]]
## Обозначения

- **Входные данные**: `x=[x1,x2]`
- **Истинное значение (цель)**: `y`
- **Веса**:
    - Вход в скрытый слой: *w11, w12, w21, w22* (веса между входным и скрытым слоями)
	- Скрытый в выходной слой: *w31, w32​* (веса между скрытым и выходным слоями)

### Вперед-проход (Forward Pass)

- #### 1. Вычисление выходов скрытого слоя

![[Pasted image 20240724232210.png]]

- #### **Вычисление выхода выходного слоя**
![[Pasted image 20240724232343.png]]

- #### Вычисление ошибки

Допустим, используем квадратичную функцию потерь:
![[Pasted image 20240724233938.png]]
### Обратное распространение (Backpropagation)

Цель обратного распространения — **вычислить градиенты ошибки относительно каждого веса** и обновить веса для минимизации ошибки.

### Пример одного компонента градиента

![[Pasted image 20240813192416.png]]

`C_0` - значение ошибки между результатом и желаемым выводом
`z_L` - результат работы одного нейрона - произведение `wx` + смещение `b`

![[Pasted image 20240813193510.png]]

![[Pasted image 20240813193605.png]]

