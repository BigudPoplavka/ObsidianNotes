#Машинное_обучение #Алгоритмы #Функция 
[[Взвешенное голосование]] 
[[Бустинг]]

При решении сложных задач классификации, регрессии, прогнозирования часто оказывается, что ни один из алгоритмов не обеспечивает желаемого качества восстановления зависимости. В таких случаях имеет смысл строить композиции алгоритмов, в которых ошибки отдельных алгоритмов взаимно компенсируются. Наиболее известные примеры композиций – **простое и взвешенное голосование.**

Рассматривается задача обучения по прецедентам <_X_, _Y_, _y_∗, _X_ℓ>, где 
- _X – пространство объектов; 
- _Y_ – множество ответов;
- _y_∗: _X_ → _Y_ – неизвестная целевая зависимость; 
- _X_ℓ = (_x_1, . . . , _x_ℓ) – обучающая выборка; 
- _Y_ℓ = (y1, . . . , _y_ℓ) – вектор ответов на обучающих объектах, 
- _yi_ = _y_∗(_xi_). 

Требуется построить алгоритм _a_: _X_ → _Y_, аппроксимирующий целевую зависимость _y_∗ на всём множестве _X_.
Наряду с множествами _X_ и _Y_ вводится вспомогательное множество _R_, называемое пространством оценок.

 Рассматриваются алгоритмы, имеющие вид суперпозиции _a_(_x_) = _C_(_b_(_x_)), где функция _b_: _X_ → _R_ называется алгоритмическим оператором, функция _C_: _R_ → _Y_ – решающим правилом. Многие алгоритмы классификации имеют именно такую структуру: сначала вычисляются оценки принадлежности объекта классам, затем решающее правило переводит эти оценки в номер класса. Значением оценки _b_(_x_) может быть вероятность принадлежности объекта _x_ классу, расстояние от объекта до разделяющей поверхности, степень уверенности классификации и т. п.

**_Композицией_** _T_ **_алгоритмов_** _at_(_x_) = _C_(_bt_(_x_)), _t_ = 1, . . . , _T_ называется суперпозиция алгоритмических операторов _bt_: _X_ → _R_, корректирующей операции _F_ : _RT_ → _R_ и решающего правила _C_: _R_ → _Y_:

- #### a(x) = C F(b1(x), . . . , bT(x)), x ∈ X 

Алгоритмы _at_ , а иногда и операторы _bt_ , называют базовыми алгоритмами. Суперпозиции вида _F_(_b_1, . . . , _bT_) являются отображениями из _X_ в _R_, то есть, опять-таки, алгоритмическими операторами.

Пространство оценок _R_ вводится для того, чтобы **расширить множество допустимых корректирующих операций.** Можно было бы определить корректирующую операцию и как отображение _F_: _YT_ → _Y_ , то есть **комбинировать непосредственно ответы базовых алгоритмов**. Однако в задачах классификации, когда множество _Y_ конечно, число «разумных» корректирующих операций такого вида невелико, **что ограничивает возможность оптимизаци** и _F_ под конкретную задачу. Если же комбинировать ответы алгоритмических операторов, то операция _F_ получает на вход оценки принадлежности объекта классам – более точную информацию, не огрублённую решающим правилом.
## Сильные ученики

учитывая достаточное число данных, способен реализовать произвольно хороший классификатор с высокой вероятностью, то есть для каждого _P_, _f_ ∈ _F_, _ε_ ≥0 и _δ_ ≤ 1/2, он выводит с вероятностью не менее 1−_δ_ классификатор _b_: X→{−1,+1}, удовлетворяющий условию `P[b(x)≠f(x)] ≤ ε`  с полиномиальной сложностью по времени.
## Слабые ученики

определяется так же, как сильный, но с более слабыми условиями по _ε_ и _δ_. Для конкретной пары (а не для всех) ε0≥0 и δ0≤1/2, слабый ученик реализует, с вероятностью не меньше чем 1−δ, классификатор  
_b_: _X_→{−1,+1}, удовлетворяющий условию `P[b(x)≠f(x)] ≤ ε0`

