#машинное_обучение #глубокое_обучение #CV #БПЛА

[[Бенчмарк]]

`Zhedong Zheng Yunchao Wei Yi Yang∗ SUSTech-UTS Joint Centre of CIS, Southern University of Science and Technology ReLER, AAII, University of Technology Sydney zhedong.zheng@student.uts.edu.au,yunchao.wei@uts.edu.au,yi.yang@uts.edu.au`
## Краткий обзор

[[Обзор University-1652 A Multi-view Multi-source Benchmark for Drone-based Geo-localization]]
## Текст

![[Pasted image 20240821115011.png]]

Мы рассматриваем проблему геолокализации с перекрестных видов. **Основная задача — изучить надежную функцию при больших изменениях точек обзора**. Существующие бенчмарки могут помочь, но ограничены в количестве точек обзора. Обычно предоставляются пары изображений, содержащие две точки обзора, например,
спутниковую и наземную, что может поставить под угрозу обучение признакам. Помимо камер телефонов и спутников, в этой статье мы утверждаем, что дроны могут служить третьей платформой для решения проблемы геолокализации. В отличие от традиционных изображений с земли, изображения с дронов встречают меньше препятствий, например, деревьев, и обеспечивают комплексный вид при полете вокруг целевого
места. Чтобы проверить эффективность платформы дронов, мы представляем новый многовидовой многоисточниковый бенчмарк для геолокации на основе дронов, названный University-1652. University-1652 содержит данные с трех платформ, т. е. синтетических дронов, спутников и наземных камер 1652 университетских зданий по всему миру. Насколько нам известно, University-1652 является первым набором данных геолокации на основе дронов и позволяет выполнять две новые задачи, т. е. локализацию цели с помощью дрона и навигацию дрона. Как следует из названия, локализация цели с помощью дрона предназначена для прогнозирования местоположения целевого места с помощью изображений с дрона. С другой стороны, при наличии запроса на
изображения со спутника навигация дрона заключается в том, чтобы направить дрон в интересующую область в запросе. **==Мы используем этот набор данных для анализа различных готовых функций CNN и предлагаем надежную базовую линию CNN на этом**
**сложном наборе данных. Эксперименты показывают, что University-1652**
**помогает модели изучать инвариантные к точке зрения функции, а также имеет**
**хорошую способность к обобщению в реальных сценариях==**.
### 1 ВВЕДЕНИЕ

Возможности для геолокализации с перекрестными видами огромны, что может позволить выполнять последующие задачи, такие как сельское хозяйство, аэрофотосъемка, навигация, обнаружение событий и точная доставка .
Большинство предыдущих работ рассматривают проблему геолокализации как подзадачу поиска изображений . Учитывая один запрос
изображение, снятое в одном представлении, система стремится найти наиболее релевантные изображения в другом представлении среди крупных кандидатов (галерея).
Поскольку кандидаты в галерее, особенно изображения с воздуха,
аннотированы географическим тегом, мы можем предсказать локализацию
целевого места в соответствии с геотегом результатов поиска.
**В общем, ключ к кросс-видовой геолокализации заключается в изучении**
**дискриминативного представления изображения, которое инвариантно к визуальным**
**изменениям внешнего вида, вызванным точками обзора.** 

В настоящее время большинство существующих наборов данных обычно предоставляют пары изображений и фокусируются на сопоставлении изображений с двух разных платформ, например, камер телефонов и спутников. Как показано на рисунке 1 (a) и (b), большая визуальная разница между двумя изображениями, т. е. изображением с земли и изображением со спутника, сложна для сопоставления даже для человека. Ограниченные
две точки обзора в обучающем наборе также могут поставить под угрозу модель
для изучения **==инвариантного к точке обзора признака==**. В свете вышеизложенных обсуждений важно (1) ввести набор данных с несколькими видами для изучения инвариантных к точке обзора характеристик и устранения разрыва во внешнем виде, и (2) разработать эффективные методы, которые в полной мере используют богатую информацию, содержащуюся в данных с несколькими видами. С недавней разработкой дронов мы показываем, что дрон может служить основной платформой сбора данных
для геолокации с перекрестными видами (см. рисунок 1 (c) и (d)). Интуитивно,
данные с дрона более предпочтительны, поскольку дроны могут быть мотивированы на сбор богатой информации о целевом месте. При полете
вокруг целевого места дрон может предоставлять комплексные
виды с небольшим количеством препятствий. Напротив, обычные изображения с земли,
включая панораму, неизбежно могут сталкиваться с преградами, например,
деревьями и окружающими зданиями. Однако крупномасштабные реальные изображения с дрона трудно собирать из-за высокой стоимости и проблем с конфиденциальностью. В свете недавней практики использования синтетических обучающих данных мы предлагаем
многовидовой набор данных с несколькими источниками под названием University-1652, содержащий синтетические изображения с дронов. 

University-1652 представлен в нескольких аспектах. Во-первых, он содержит многовидовые изображения для каждого целевого места. Мы манипулируем движком дронов для моделирования изображений
различных точек обзора вокруг цели, что приводит к 54 изображениям дронов для каждого места в нашем наборе данных. Во-вторых, он содержит данные
из нескольких источников. Помимо изображений с дронов, мы также собираем
спутниковые изображения и изображения с земли в качестве эталона. В-третьих,
он крупномасштабный, содержит в общей сложности 50 218 обучающих изображений и имеет 71,64 изображения на класс в среднем. Изображения в бенчмарке
сняты с 1652 зданий 72 университетов. Более подробные описания будут даны в Разделе 3. **==Наконец, University-1652 позволяет выполнять две новые задачи, а именно: локализацию цели с помощью дрона и навигацию с помощью дрона.==**

**Задача 1**: локализация цели с помощью дрона. (Дрон → Спутник)
Для одного изображения или видео с помощью дрона задача направлена ​​на поиск наиболее похожего изображения со спутника для локализации целевого здания на
спутниковом снимке.

**Задача 2**: навигация с помощью дрона. (Спутник → Дрон)Для одного изображения со спутника дрон намеревается найти наиболее подходящее место (изображения с помощью дрона), мимо которого он пролетел. **Согласно истории полета, дрон можно было бы направить обратно к целевому месту.** 

В эксперименте мы рассматриваем две задачи как проблемы поиска изображений с перекрестными видами и **==сравниваем общий признак, обученный на чрезвычайно больших наборах данных, с признаком, инвариантным к точке обзора,**
**изученным на предлагаемом наборе данных.==** Мы также оцениваем три базовые модели и
три различных термина потерь, включая 
- **контрастную потерю,
- **триплетную потерю**
- **потерю экземпляра**. 

Помимо обширной оценки базового метода, мы также тестируем изученную модель на реальных изображениях с дронов, чтобы оценить масштабируемость изученного признака. Наши результаты показывают, что University-1652 помогает модели
обучиться инвариантному к точке зрения признаку и приближается на шаг к
практике. Наконец, набор данных University-1652, а также код для
базового бенчмарка будут предоставлены в открытый доступ для добросовестного использования 
### 2 СВЯЗАННЫЕ РАБОТЫ
#### 2.1 Обзор набора данных геолокации

Большинство предыдущих наборов данных геолокации основаны на парах изображений и целевом сопоставлении изображений с двух разных платформ, таких как
камеры телефонов и спутники. Одна из самых ранних работ
предлагает использовать общедоступные источники для создания пар изображений для
изображений с земли и с воздуха. Она состоит из 78 тыс. пар изображений с двух видов, т. е. 45◦ вид с высоты птичьего полета и вид с земли. Позже,
в похожем духе, Тиан и др. собирают пары изображений для
локализации в городах. По-другому, они утверждают, что здания могут играть
важную роль в проблеме локализации в городах, поэтому они включают
обнаружение зданий во весь конвейер локализации. Кроме того,
два последних набора данных, т. е. CVUSA и CVACT, изучают проблему сопоставления панорамного изображения с земли и изображения со спутника. Он может проводить локализацию пользователя, когда глобальная система позиционирования (GPS) недоступна. Основное различие между первыми двумя наборами данных и последними двумя наборами данных заключается в том, что последние два набора данных фокусируются на локализации пользователя, который делает фотографию. Напротив, первые два набора данных и наш предлагаемый набор данных фокусируются на локализации цели на фотографии. Поэтому несколько видов на цель более благоприятны, что может заставить модель понять структуру цели, а также помочь облегчить сложность сопоставления. Однако существующие наборы данных обычно предоставляют два вида целевого места. В отличие от существующих наборов данных, предлагаемый набор данных University-1652 включает больше видов цели для повышения инвариантного к точке зрения
обучения признаку
#### 2.2 Глубоко изученный признак для геолокализации

Большинство предыдущих работ рассматривают геолокализацию как проблему поиска изображений. **Ключом к геолокализации является изучение инвариантного представления точки зрения, которое призвано устранить разрыв между изображениями с разных точек зрения.** С развитием глубоко изученной модели [[Сверточные нейросети]]  (CNN)
широко применяются для извлечения визуальных признаков. Одна из линий работ
сосредоточена на метрическом обучении и построении общего пространства для изображений, собранных с разных платформ. Воркман и др. показывают, что
классификационная CNN, предварительно обученная на наборе данных Place, может
быть очень дискриминантной сама по себе без явной тонкой настройки 
Контрастная потеря, увеличивающая расстояние между положительными парами,
может дополнительно улучшить результаты геолокализации. **Недавно**
**Лю и др. предложили потерю стохастического притяжения и отталкивания**
**(SARE), минимизирующую расхождение KL между изученными**
**и фактическими распределениями .**
[[SARE]], [[Расхождение Кульбака-Лейблера]]

**Другая линия работ фокусируется на**
**проблеме пространственного несоответствия при сопоставлении наземных и воздушных данных.** Во и др. оценивают различные структуры сетей и предлагают
потерю регрессии ориентации для обучения сети, учитывающей ориентацию
. Чжай и др. используют карту семантической сегментации для помощи в
семантическом выравнивании , а Ху и др. вставляют слой [[NetVLAD]]
**для извлечения отличительных признаков**. Кроме того, Лю и др. предлагают
**сиамскую сеть** для явного включения пространственных подсказок, т. е. карт ориентации, в обучение. Аналогично, Ши и др. предлагают
слой с пространственным знанием для дальнейшего улучшения производительности локализации. В этой статье, поскольку каждое местоположение имеет ряд обучающих
данных с разных точек зрения, мы могли бы обучить классификационную сверточную нейронную сеть в качестве базовой модели. При тестировании мы используем обученную модель для извлечения визуальных признаков для изображений запроса и галереи. Затем мы проводим сопоставление признаков для быстрой геолокации.
### 3 НАБОР ДАННЫХ UNIVERSITY-1652
#### 3.1 Описание набора данных

В этой статье мы собираем спутниковые изображения, изображения с дронов
с помощью имитированных камер дронов и изображения с земли для
каждого местоположения. Сначала мы выбираем 1652 архитектуры 72 университетов
по всему миру в качестве целевых местоположений. Мы не выбираем достопримечательности в качестве цели. Две основные проблемы: во-первых, достопримечательности обычно содержат дискриминационные стили архитектуры, которые могут вносить некоторые неожиданные предубеждения; во-вторых, дрону обычно запрещено летать вокруг достопримечательностей. Исходя из двух проблем, мы выбираем здания на территории кампуса в качестве цели, что ближе к
реальной практике. из разных источников. Вместо того, чтобы собирать данные и затем находить связи между различными источниками, мы начинаем со сбора
метаданных. Сначала мы получаем метаданные университетских зданий из
Википедии 1, включая названия зданий и принадлежность к университетам.
Во-вторых, мы кодируем название здания в точное геолокационное положение,
то есть широту и долготу, с помощью Google Map. Мы отфильтровываем здания с неоднозначными результатами поиска, и остается 1652 здания.
В-третьих, мы проецируем геолокационные положения в Google Map, чтобы получить
спутниковые изображения. Для изображений с дрона, из-за
недоступной стоимости реального полета, мы используем 3D-модели, предоставленные Google Earth, для имитации реальной камеры дрона.
3D-модель также обеспечивает манипулятивные точки обзора. Чтобы включить изменения масштаба и получить комплексные точки обзора, **мы устанавливаем**
**кривую полета в виде спирали (см. Рисунок 2(a)) и записываем**
**видео полета с частотой 30 кадров в секунду. Камера облетает**
**цель с тремя кругами. Высота постепенно уменьшается с 256**
**метров до 121,5 метра, что близко к высоте полета дрона в**
**реальном мире .**
Для изображений с земли мы сначала собираем данные с изображений улиц вблизи целевых зданий с Google Map. В частности,
мы вручную собираем изображения в разных аспектах здания
(см. Рисунок 2(b)).

![[Pasted image 20240821120003.png]]

Однако для некоторых зданий нет фотографий улиц из-за доступности, т. е. большинство изображений улиц собираются с камеры на крыше автомобиля. **Чтобы решить эту**
**проблему, мы, во-вторых, вводим один дополнительный источник, т. е. поисковую**
**систему изображений. Мы используем название здания в качестве ключевых слов для получения**
**соответствующих изображений. Однако одно неожиданное наблюдение заключается в том, что полученные изображения часто содержат много шумовых изображений, включая ==внутренние**
**среды и дубликаты. Поэтому мы применяем модель ResNet-18,**
**обученную на наборе данных Place ,== для обнаружения внутренних изображений и следуем настройкам в, чтобы удалить идентичные изображения, которые принадлежат**
**двум разным зданиям**. Таким образом, мы собираем 5580 изображений с видом на улицу и 21099 изображений с общим видом из Google Map и
Google Image соответственно. Следует отметить, что изображения, собранные
из Google Image, служат только дополнительным обучающим набором, но и тестовым набором.
Наконец, каждое здание имеет 1 изображение со спутника, 1 видео с дрона и 3,38 реальных изображений с улицы в среднем. Мы обрезаем
изображения из видео с дрона каждые 15 кадров, в результате чего получаем
54 изображения с дрона. В целом, каждое здание имеет всего 58,38
эталонных изображений. Кроме того, если мы используем дополнительные данные, полученные Google,
мы получим 16,64 изображений с земли на здание для обучения.

По сравнению с существующими наборами данных (см. Таблицу 1), мы суммируем
новые функции в University-1652 в следующих аспектах:
1) Многоисточник: University-1652 содержит данные с трех
разных платформ, т. е. спутников, дронов и камер телефонов. Насколько
нам известно, University-1652 является первым набором данных геолокации,
содержащим изображения с дронов.
2) Многовидовой: University-1652 содержит данные с разных
точек обзора. Изображения с земли собираются с разных
граней целевых зданий. Кроме того, синтетические изображения с дронов
захватывают целевое здание с разных расстояний и ориентаций.
3) Больше изображений на класс: в отличие от существующих наборов данных,
которые предоставляют пары изображений, University-1652 содержит в среднем 71,64 изображения
на местоположение. Во время обучения больше данных из нескольких источников и нескольких представлений может помочь модели понять целевую
структуру, а также изучить инвариантные к точке зрения признаки. На этапе тестирования больше изображений запросов также позволяют настроить несколько запросов. В эксперименте мы показываем, что несколько запросов могут привести к более точной локализации цели.
#### 3.2 Evaluation Protocol 

Всего в University-1652 1652 здания. 1402 здания содержат все три вида, т. е. спутниковые, с беспилотника и с земли, и 250 зданий, у которых нет ни 3D-модели, ни изображений с улицы. Мы равномерно разделили 1402 здания на обучающий и тестовый наборы, содержащие 701 здание 33 университетов, 701 здание остальных 39 университетов. Мы отмечаем, что в обучающем и тестовом наборах нет перекрывающихся университетов. Остальные 250 зданий добавлены в галерею в качестве отвлекающих факторов. Более подробная статистика представлена ​​в таблице 2. Несколько предыдущих наборов данных используют Recall@K, значение которого равно 1, если первое сопоставленное изображение появилось до K-го изображения. Recall@K чувствителен к положению первого сопоставленного изображения и подходит для тестового набора только с одним истинно сопоставленным изображением в галерее. Однако в нашем наборе данных есть несколько истинно сопоставленных изображений с разных точек зрения в галерее. Recall@K не может отразить результат сопоставления остальных изображений ground-truth. Поэтому мы также принимаем среднюю точность (AP). Средняя точность (AP) — это площадь под кривой PR (точность-Recall), учитывающей все изображения ground-truth в галерее. Помимо Recall@K, мы вычисляем AP и сообщаем среднее значение AP всех запросов.
### 4 CROSS-VIEW IMAGE MATCHING 

Сопоставление изображений с разными видами можно сформулировать как метрическую задачу обучения. Цель состоит в том, чтобы сопоставить изображения из разных источников с общим пространством. **В этом пространстве вложения одного и того же местоположения должны быть близки, в то время как вложения разных местоположений должны быть разделены.** 
#### 4.1 Представления признаков 

Не существует «стандартных» представлений признаков для набора данных с несколькими источниками и несколькими видами, который требует надежных признаков с хорошей масштабируемостью для различных типов входных изображений. В этой работе мы в основном сравниваем два типа признаков: (1) общие признаки глубокого обучения, обученные на чрезвычайно больших наборах данных, таких как ImageNet, Place-365 и SfM-120k; (2) изученный признак на нашем наборе данных. Для справедливого сравнения основой всех сетей является **ResNet-50**, если не указано иное. Более подробная информация приведена в разделе 5.2. Далее мы описываем метод обучения на наших данных в следующем разделе.

![[Pasted image 20240821120308.png]]
#### 4.2 Network Architecture and Loss Function 

Изображения из разных источников могут иметь разные низкоуровневые шаблоны, поэтому мы обозначаем три разные функции` Fs , Fg и Fd ,` которые проецируют входные изображения со спутников, наземных камер и дронов на высокоуровневые признаки. В частности, для изучения проекционных функций мы следуем общепринятой практике в и принимаем двухветвевую CNN в качестве одной из наших базовых структур. Чтобы проверить приоритет изображений, полученных с дрона, по сравнению с изображениями, полученными с земли, мы вводим две базовые модели для разных входных данных (см. Рисунок 3 (I), (II)). **Поскольку наш набор данных содержит данные из трех разных источников, мы также расширяем базовую модель до трехветвевой CNN**, чтобы полностью использовать аннотированные данные (см. Рисунок 3 (III)). Чтобы изучить семантические отношения, нам нужна одна цель для преодоления разрыва между различными представлениями. Поскольку наши наборы данных предоставляют несколько изображений для каждого целевого места, мы могли бы рассматривать каждое место как один класс для обучения модели классификации. В свете последних разработок в области двунаправленного поиска на языке изображений мы принимаем одну потерю классификации, называемую потерей экземпляра, для обучения базовой линии. Основная идея заключается в том, что общий классификатор может принудительно сопоставлять изображения из разных источников с одним общим пространством признаков. Мы обозначаем xs , xd и xд как три изображения местоположения c, где xs , xd
и xд являются изображением со спутника, изображением с дрона и изображением с земли соответственно. Учитывая пару изображений {xs , xd} из двух видов,
базовая потеря экземпляра может быть сформулирована как:

![[Pasted image 20240821120435.png]]

где Wshar e — вес последнего слоя классификации. p(c) — это
прогнозируемая возможность правильного класса c. В отличие от обычной потери классификации, общий вес Wshar e обеспечивает мягкое
ограничение на высокоуровневые признаки. Мы могли бы рассматривать Wshar e как
один линейный классификатор. После оптимизации различные пространства признаков
сопоставляются с пространством классификации. В этой статье мы дополнительно расширяем базовую потерю экземпляра для обработки данных из нескольких источников.
Например, если предоставляется еще одно представление, нам нужно включить только
один еще один критериальный термин:

![[Pasted image 20240821120504.png]]
### 5 ЭКСПЕРИМЕНТ

#### 5.1 Подробности реализации

Мы принимаем ResNet-50, предварительно обученную на ImageNet в качестве нашей
основной модели. 
- **Мы удаляем исходный классификатор для ImageNet и вставляем один 512-мерный полностью связанный слой и один классификационный слой после слоя объединения**.
- Модель обучается стохастическим градиентным спуском с импульсом 0,9.
- Скорость обучения составляет 0,01 для новых добавленных слоев и 0,001 для остальных слоев. 
- Скорость отсева составляет 0,75. 
- Во время обучения размеры изображений изменяются до 256 × 256 пикселей. 

Мы выполняем простое увеличение данных, например, горизонтальное переворачивание. Для спутниковых изображений мы также проводим случайное вращение. При тестировании мы используем обученную CNN для извлечения соответствующих признаков
для разных источников. Косинусное расстояние используется для расчета
сходства между запросом и изображениями-кандидатами в галерее. Окончательный результат поиска основан на рейтинге сходства. Если не указано иное, мы используем Model-III, которая полностью использует аннотированные данные в качестве базовой модели. Мы также разделяем веса Fs и Fd, поскольку два источника с аэрофотоснимков имеют некоторые схожие шаблоны 
#### 5.2 Результаты геолокализации

Чтобы оценить несколько настроек геолокализации, мы предоставляем запрос
изображений из источника A и извлекаем соответствующие изображения в галерее B.
Мы обозначаем тестовую настройку как A → B.
Общие признаки против изученных признаков. Мы оцениваем две категории признаков: 1) общие признаки CNN. Некоторые предыдущие работы показывают, что модель CNN, обученная либо на ImageNet, либо на PlaceNet, сама изучила отличительный признак.
Мы извлекаем признак перед конечным слоем классификации.
Размер признака составляет 2048. Кроме того, мы также тестируем широко используемую
модель распознавания мест, основой которой является ResNet-101. 2)
признаки CNN, изученные на нашем наборе данных. Поскольку мы добавляем один полностью связанный слой перед слоем классификации, наш конечный признак
составляет 512-мерный. Как показано в таблице 3, наша базовая модель достигает гораздо
лучшей производительности с более короткой длиной признака, что подтверждает
эффективность предлагаемой базовой линии. Запрос наземного вида против запроса беспилотного вида. Мы утверждаем, что изображения беспилотного вида более предпочтительны по сравнению с изображениями наземного вида,
поскольку изображения беспилотного вида сделаны с аналогичной точки обзора, т. е.
с высоты птичьего полета, со спутниковыми снимками. Между тем, изображения беспилотного вида могут избегать препятствий, например, деревьев, что часто встречается на изображениях наземного вида. Чтобы проверить это предположение, мы обучаем базовую модель и извлекаем визуальные особенности трех видов данных.
Как показано в Таблице 4, при поиске соответствующих изображений со спутникового вида запрос беспилотного вида превосходит запрос наземного вида.
Наша базовая модель с использованием запроса беспилотного вида достигла 58,49%
Rank@1 и 63,13% точности AP. Множественные запросы. Кроме того, в реальном сценарии одно изображение не может предоставить исчерпывающее описание целевого
здания. Пользователь может использовать несколько фотографий целевого
здания с разных точек обзора в качестве запроса. Например, мы могли бы управлять полетом дрона вокруг целевого места, чтобы сделать несколько фотографий. Мы оцениваем настройку множественных запросов, напрямую усредняя функции запроса. Поиск с несколькими запросами с видом с дрона
обычно обеспечивает более высокую точность с улучшением Rank@1 и AP примерно на 10% по сравнению с настройкой с одним запросом
(см. Таблицу 4). Кроме того, целевая локализация с использованием запросов с видом с дрона по-прежнему достигает лучшей производительности, чем запросы с видом с земли

![[Pasted image 20240821121029.png]]

с большим отрывом. Мы предполагаем, что запрос наземного вида
не работает хорошо в настройке одного запроса, что также ограничивает
улучшение производительности в настройке множественных запросов.
Помогают ли данные с несколькими видами обучению признакам, инвариантным к точке обзора? Да. Мы фиксируем гиперпараметры и только изменяем
количество изображений с дрона в обучающем наборе. Мы обучаем пять моделей с n изображениями с дрона на класс, где n ∈ {1, 3, 9, 27, 54}. Как показано на рисунке 4, когда мы постепенно включаем больше обучающих изображений с дрона
с разных точек обзора, точность Rank@1 и точность AP увеличиваются.
Работает ли обученная модель на реальных данных? Да. Из-за стоимости сбора реальных видео с дрона, здесь мы проводим качественный эксперимент. Мы собираем одно реальное видео с дрона в формате 4K University-X с Youtube, предоставленное автором. University-X является одним из учебных заведений в тестовом наборе, и базовая модель
не видела ни одного образца из University-X. Мы обрезаем изображения из
видео, чтобы оценить модель. На рисунке 6 мы показываем два результата поиска, то есть, Real Drone → Synthetic Drone, Real Drone → Satellite. Первый результат поиска заключается в проверке того, хорошо ли наши синтетические данные моделируют изображения в реальных камерах дронов. Мы показываем 5 самых похожих изображений в тестовом наборе, полученных нашей базовой моделью. Это демонстрирует, что визуальная характеристика запроса реального вида дрона близка к характеристике наших синтетических изображений вида дрона. Второй результат по Real Drone → Satellite заключается в проверке обобщения нашей обученной модели на реальных данных вида дрона. Мы видим, что базовая модель обладает хорошей способностью к обобщению и также работает на реальных изображениях с дрона для локализации цели с дрона.
Все истинно сопоставленные спутниковые изображения извлекаются в топ-5
списка рейтинга. **Визуализация**. Для дополнительной качественной оценки мы показываем результаты извлечения нашей базовой моделью на тестовом наборе University-1652
(см. Рисунок 5). Мы видим, что базовая модель способна находить
релевантные изображения с разных точек зрения. Для ложно сопоставленных
изображений, хотя они не совпадают, они имеют схожий
структурный шаблон с изображением запроса.
#### 5.3 Исследование абляции и дальнейшее обсуждение

Влияние целей потери. Триплетная потеря и контрастная потеря
широко применялись в предыдущих работах, а взвешенная
мягкая краевая потеря триплета развернута в  Мы оцениваем эти
три потери по двум задачам, т. е. Дрон → Спутник и Спутник →
Дрон и сравниваем три потери с потерей экземпляра, используемой в нашей
базовой линии. Для справедливого сравнения все потери обучаются с помощью одной и той же магистральной модели и используют только данные с дрона и спутниковой линии в качестве обучающего набора. Для триплетной потери мы также пробуем два общих
значения маржи {0,3, 0,5}. Кроме того, политика жесткой выборки также применяется к этим базовым методам во время обучения . Как показано в Таблице 5, мы наблюдаем, что модель с потерей экземпляра обеспечивает лучшую производительность, чем триплетная потеря и контрастная потеря в обеих задачах. Эффект совместного использования весов. В нашей базовой модели Fs и Fd разделяют веса, поскольку два воздушных источника имеют некоторые схожие закономерности. Мы также тестируем модель без совместного использования весов (см. Таблицу 6). Производительность обеих задач падает. Основная причина заключается в том, что ограниченные спутниковые изображения (одно спутниковое изображение на местоположение) склонны к переобучению отдельной ветвью CNN. При совместном использовании весов, изображения с дронов могут помочь упорядочить модель, и модель, таким образом, достигает лучшего Rank@1 и точности AP.

**Влияние размера изображения.** Изображения со спутников содержат мелкозернистую информацию, которую можно сжать с помощью небольшого размера обучения. Поэтому мы пытаемся увеличить размер входного изображения и обучить модель с глобальным средним объединением. Размерность
конечного признака по-прежнему составляет 512. Как показано в таблице 7, когда мы увеличиваем
размер ввода до 384, точность как задачи, так и цели
локализации вида с дрона (Дрон → Спутник) и навигации дрона (Спутник → Дрон) увеличивается. Однако, когда мы увеличиваем размер до 512,
производительность падает. Мы предполагаем, что больший размер ввода слишком
отличается от размера предварительно обученного веса на ImageNet, который
составляет 224 × 224. В результате размер ввода 512 не работает хорошо.

**Разные базовые модели.** Мы оцениваем три различные базовые
модели, как обсуждалось в Разделе 4. Как показано в Таблице 8, есть
два основных наблюдения: 1). Модель II достигла лучшего ранга 1
и точности AP для навигации дронов (спутник → дрон). Это не
удивительно, поскольку Модель II обучена только на данных с дрона и
спутника. 2). Модель III, которая полностью использует все аннотированные
данные, достигла наилучшей производительности в трех из всех
четырех задач. Она может служить сильной базовой линией для нескольких задач.

**Предлагаемая базовая линия на другом эталоне.** Как показано в Таблице 9, мы также оцениваем предлагаемую базовую линию на одном широко используемом
двухпозиционном эталоне, например, CVUSA. Для справедливого сравнения мы
также принимаем 16-слойную VGG в качестве базовой модели. Мы
не собираемся продвигать самые современные характеристики, а хотим показать
гибкость предлагаемой базовой линии, которая также может работать на
обычном наборе данных. Поэтому мы не проводим трюки, такие как
выравнивание изображений или ансамбль признаков. Наша интуиция заключается в том, чтобы предоставить сообществу одну простую и гибкую базовую линию для дальнейшей оценки. По сравнению с обычной сиамской сетью
с потерей триплетов, предлагаемый метод можно легко расширить
на обучающие данные из N разных источников (N ≥ 2). Пользователям
нужно только изменить количество ветвей CNN. Несмотря на простоту,
эксперимент подтверждает, что предлагаемый метод может служить
сильной базовой линией и имеет хорошую масштабируемость по отношению к реальным образцам.
#### Переносим обучение из University-1652 на наборы данных малого масштаба.

Мы оцениваем способность к обобщению базовой модели на двух наборах данных малого масштаба, т. е. Oxford и Pairs. Oxford и
Pairs — два популярных набора данных для распознавания мест. Мы напрямую оцениваем нашу модель на этих двух наборах данных без тонкой настройки. Кроме того, мы
также сообщаем результаты по пересмотренным наборам данных Oxford и Paris (обозначаемым как ROxf и RPar). В отличие от общего признака, обученного на
ImageNet, изученный признак на University-1652 показывает лучшую способность к обобщению. В частности, мы пробуем две разные ветви, т. е. Fs и Fд, для извлечения признаков. Fs и Fд разделяют высокоуровневое пространство признаков, но обращают внимание на различные низкоуровневые шаблоны входных данных с разных платформ. Fs обучается на спутниковых изображениях и изображениях, полученных с помощью дронов, в то время как Fд обучается на изображениях, полученных с земли. Как показано в Таблице 10, Fд достиг лучших результатов, чем Fs. Мы предполагаем, что есть две основные причины. Во-первых, тестовые данные в Оксфорде и Парсе собраны с Flickr, что ближе к изображениям Google Street View и изображениям, полученным из Google
Image в данных наземного обзора. Во-вторых, F уделяют больше внимания
изменениям вертикальной точки обзора вместо горизонтальных изменений точки обзора,
которые распространены в Оксфорде и Париже.