#Машинное_обучение #Нейросети 

[[Нейронные сети]]
[[Нейрон]]
[[Softmax]]
[[Градиент]]
## Линейный классификатор

**Линейный классификатор** — это тип модели машинного обучения, который делает предсказания **==на основе линейной комбинации входных признаков==**. Его основная задача — **разделить пространство признаков на области**, каждая из которых соответствует одному из классов **(Плоскости n-1 мерные).** Модель можно представить как линейное уравнение:

**y = W⋅X+b** 

Где:

- `X` — вектор входных признаков.
- `W` — матрица весов.
- `b` — смещение (bias).
- `y` — результат линейной комбинации, который затем интерпретируется как вероятность принадлежности к классу.
![[Pasted image 20240812135746.png]]

![[Pasted image 20240809112712.png]]
## Как работает линейный классификатор?

1. **Линейная комбинация признаков:** Для каждого класса модель вычисляет линейную комбинацию признаков. **==Результатом будет число==**, которое мы можем интерпретировать как "оценку" принадлежности к этому классу.

	Делается это за счет матричного умножения вектора входов `x` на матрицу весов `w` + смещения `b`
    
2. **Функция активации:** **==Чтобы перевести эти оценки в вероятности==**, применяют функцию активации. Самая популярная функция — **softmax.**

	**Softmax** — это функция активации, которая принимает вектор действительных чисел и преобразует их в распределение вероятностей. То есть на выходе мы получаем вектор, сумма значений которого равна 1, и каждое значение можно интерпретировать как вероятность принадлежности к определённому классу. Softmax делает следующее:
	
	Мягкая максимизация заключается в том, что при нормализации значений `y` (результат перемножения весов и входов) до значений от **0 до 1** самые большие значения будут быстро приближены к 1, а наименьшие к 0. 
	
	Это достигается за счет использования **экспоненты** в силу ее характера быстрого роста при небольшом увеличении данных.
    
3. **Классификация:** Класс с наибольшей вероятностью будет считаться предсказанным.
## Обучение

Обучение классификатора состоит в подборе таких значений `w` и `b`, при которых результат матричных умножений `x * w + b` после **softmax** преобразуется в правильную максимальную вероятность для каждого соответствующего `x_i` из всех входных (**==при одинаковых==** `w, b`) 
### Принцип максимального правдоподобия

В конечном итоге мы оптимизируем `w, b` так, чтобы максимизировать `p(data)` - то есть предсказанные вероятности для всего датасета. Однако вместо этого также используют **Negative Log-Likelihood** - взять минус логарифм этой функции с целью прийти от произведения к сумме (проще)

![[Pasted image 20240809130006.png]]

Такая ф-ция потерь (оптимизации) это **==Кросс-энтропия==**

В развернутом виде 

![[Pasted image 20240809132354.png]]

- `w * x + b`  - внутренние преобразования (матричное умножение + биас) 
- `gt_s` - ground truth для сэмпла s - записанный лейбл в результате
## Регуляризация

К функции могут применяться дополнительные параметры или даже функции для того, чтобы повлиять на ее поведение, например повысить ее гладкость или привести к единственности: 
> например `w` или `b` **+- константа** дает нам еще вариант функции
> 

Для этого можно прибегнуть к минимизации суммы квадратов весов и биасов. Это поможет сделать значение минимума единственным

![[Pasted image 20240809133736.png]]

**лямбда** - гиперпараметр

Запись `||w||` с нижним индексом 2 значить L2-норму. То есть она равна корню из суммы квадратов весов вектора` w_ii, w_ij`
## SGD Стохастический градиентный спуск

![[Pasted image 20240809135255.png]]
### 1. Преобразование входных данных

У вас есть входное изображение размером **32 x 32** пикселя с **3** каналами (RGB). Это изображение представляется в виде массива размером **32 x 32 x 3**, который затем преобразуется в вектор размерностью **3072 (то есть 32 * 32 * 3 = 3072)**
### 2. Инициализация матрицы весов

Для сети с **10** выходами (нейронами) матрица весов будет иметь размерность **3072 x 10**. Это означает, что каждый из **3072** входных признаков связан с каждым из **10** нейронов в выходном слое. В дополнение к матрице весов, у вас будет вектор смещений (bias) размером **1 x 10**.
### 3. Обработка батча данных

**a. Подготовка данных:** Вы берете батчи из датасета. Предположим, размер батча равен **128**, что означает, что у вас есть **128** изображений, каждое из которых представлено в виде вектора размером **3072**. Таким образом, ваш батч данных будет иметь размерность **128 x 3072**.

**b. Прямой проход:** Для батча из **128** изображений вы умножаете матрицу весов (размером **3072 x 10**) на входной батч (размером **128 x 3072**) и добавляете смещения (размером **1 x 10**). Результат будет матрицей размером **128 x 10**. Это и есть предсказания сети до применения функции активации.

**c. Softmax:** Затем применяете softmax к выходам сети. Softmax переводит логиты (непосредственные выходы нейронной сети) в вероятности, которые будут использоваться для оценки лосса. После применения softmax результатом будет матрица вероятностей размером 128 x 10, где каждая строка представляет собой распределение вероятностей по классам для конкретного изображения в батче.
### 4. ПРЯМОЙ ПРОХОД - Вычисление лосса

**a. Лосс-функция:** На полученных вероятностях вы вычисляете лосс-функцию (например, кросс-энтропию) по отношению к истинным меткам для каждого изображения в батче. Лосс-функция измеряет, насколько хорошо предсказанные вероятности совпадают с истинными метками. Результатом будет значение лосса для всего батча.
### 5. ОБРАТНЫЙ ПРОХОД - Обратное распространение ошибки (Backpropagation)

**a. Вычисление градиентов:** Вы вычисляете градиенты лосса **==по отношению к весам и смещениям==**, используя [[Обратное распространение ошибки]]. Эти градиенты указывают, насколько сильно лосс изменится при изменении каждого веса и смещения.

**b. Обновление параметров:** Обновляете веса и смещения, используя градиенты, полученные в предыдущем шаге, и алгоритм градиентного спуска (SGD) или его вариации (например, Adam, RMSprop).
### Резюме

1. **Вход:** Входные изображения преобразуются в векторы размерностью 3072.
2. **Матрица весов:** Размер матрицы весов — 3072 x 10.
3. **Батч:** Обработка батча размером 128 x 3072 дает матрицу предсказаний размером 128 x 10.
4. **Softmax:** Применяется к матрице предсказаний, результат — матрица вероятностей размером 128 x 10.
5. **Лосс:** Вычисляется на основе вероятностей и истинных меток.
6. **Градиенты:** Рассчитываются для весов и смещений, и используются для их обновления.

![[Pasted image 20240817164614.png]]

**Очень важен размер батча.** Если он слишком маленький, то оптимизация будет слишком прыгать в разные стороны, а если слишком большой - это слишком много времени.

> А обработка всего датасета один раз (все мини-батчи), проход градиента через весь датасет - настройка весов и биасов - **==это одна эпоха==**

## Объединение W и B

Можно для упрощения вычислений объединить матрицы весов и смещений.

![[Pasted image 20240809143149.png]]

Для этого к батчам бодавляем **столбец единиц**, а смещения (10 шт. по числу нейронов) добавим строкой в матрицу весов.
## Вычисление градиента

### Численный

![[Pasted image 20240813150446.png]]
### Аналитический

![[Pasted image 20240813150527.png]]
## Нелинейные функции активации

![[Pasted image 20240813153406.png]]

Между слоями `w_i` размещаются нелинейные [[Функции активации]]. 

![[Pasted image 20240813155435.png]]
## Формула как граф вычислений

![[Pasted image 20240813160046.png]]

![[Pasted image 20240813161914.png]]

![[Pasted image 20240813170716.png]]

