#Машинное_обучение 

**Overfitting (переобучение)** возникает, когда модель слишком сложна и начинает запоминать обучающие данные, вместо того чтобы выявлять общие закономерности, что приводит к хорошим результатам на обучающих данных, но плохим на тестовых.
## Overfitting

![[Pasted image 20240820103826.png]]

При overfitting точность на тренировочных данных будет расти при постепенном падении точности на валидации. То есть она просто приспособилась к тренировочным, и на валидационных не работает корректно.
## Методы регуляризации для борьбы с overfitting:

1. **L1 и L2 регуляризация (Ridge и Lasso регрессии):**
	    [[L-нормы]]
    - **L2-регуляризация (Ridge)** добавляет штраф в виде суммы квадратов весов параметров модели к функции потерь. Это способствует уменьшению значений весов и, соответственно, снижению сложности модели.
    - **L1-регуляризация (Lasso)** добавляет штраф в виде суммы модулей весов, что может привести к занулению некоторых параметров, тем самым выполняя отбор признаков.
2. **Dropout (для нейронных сетей):**
	    [[Дропаут]]
    - Dropout заключается в случайном отключении некоторых нейронов на этапе обучения, что помогает предотвратить зависимость от конкретных нейронов и улучшает обобщающую способность модели.
3. **Early Stopping:**
    - Остановить обучение модели на той итерации, когда ошибка на валидационном наборе начинает увеличиваться, хотя ошибка на обучающем наборе продолжает уменьшаться. Это предотвращает чрезмерное подгонку модели под обучающие данные.
4. **Data Augmentation (увеличение объема данных):**
	    [[Аугументация данных]]
    - Генерация дополнительных данных из исходных путем применения различных трансформаций (например, поворотов, масштабирования в случае изображений). Это помогает модели лучше обобщать, а не запоминать данные.
5. **Простые модели:**
    - Уменьшение сложности модели (например, уменьшение количества параметров, глубины деревьев решений, числа слоев в нейронной сети) также может предотвратить переобучение.
6. **Снижение размерности (например, PCA):**
    - Применение методов снижения размерности для уменьшения количества признаков, что может помочь модели не переусложняться.