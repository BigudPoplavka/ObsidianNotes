#Машинное_обучение #Оптимизация_управления 

[[Градиент]]
[[Гессиан]]
## Обновление параметров

### Самый простой

Обновление путем умножения `learning_rate` на градиент. Однако в этом случае даже SGD не самый оптимальный по скорости достижения минимума.
### Методы с неконстантным шагом

#### Momentum, velocity

Мы накапливаем некоторую скорость, как в физическом процессе спуска объекта. Так мы увеличиваем скорость обновления весов.
![[Pasted image 20240819094848.png]]
## Проблема седловых функций

Проблема состоит в функциях имеющих разную выпуклость по разным измерениям. Типичный пример седловой функции - это **==гиперболический тангенс==**
Проблема случается, когда градиент скатывается в локальный минимум и не может из него выбраться. Причина в том, что локально градиент обращается в 0.

![[Pasted image 20240819100443.png]]
### Решение
#### Вторые производные

Решением являются модернизации градиентных метолов, учитывающие вторые производные, **Гессиан**
### Адаптивный градиент AdaGrad

`torch.optim.Adagrad`

Искусственно изменим градиент. Будем аккумулировать квадраты градиента и по измерениям. **==Те направления, в которых градиент очень мал - мы их увеличим. А те, в которых он хочет более активно расти (в зоне седловой точки), там мы его уменьшим.==**

Это работает за счет деления на корень из аккумулированного градиента. Там, где он накопился большим - он уменьшится, а там, где был мал - их влияние усилится.

При этом **==накопление градиента идет за всю историю обучения==**, поэтому learning_rate будет только уменьшаться.

![[Pasted image 20240819103306.png]]
### RMSProp

- Использует среднее квадратов градиентом **==(второй момент)==**

`torch.optim.RMSprop`

В отличие от AdaGrad, к аккумуляции градиента применяется релаксация `rho` **==0.9 - 0.999==** и накапливает градиент только за некоторое конечное число последних шагов: **Эффект затухания:** Вместо того чтобы учитывать все градиенты, как это делает Adagrad, RMSprop использует экспоненциальное затухание для сглаживания суммируемых квадратов градиентов. Это позволяет поддерживать значение шага обучения в стабильных пределах.

RMSprop обновляет каждый параметр модели **с учетом средней величины квадратов градиента по предыдущим итерациям. Это позволяет уменьшить шаг обучения для часто обновляемых параметров и увеличить его для редко обновляемых.**

**RMSprop:** **==Адаптирует шаг обучения на основе средней величины квадратов градиентов==**, что помогает эффективно обучать модели с различными шкалами параметров.

![[Pasted image 20240819185756.png]]
![[Pasted image 20240819155927.png]]
### Adam

- Использует среднее градиентов и среднее квадратов градиентов **==(первый и второй моменты)==**

`torch.optim.Adam`

**Адаптивная скорость обучения**:  Adam адаптирует скорость обучения для каждого параметра модели индивидуально, что позволяет ему эффективно справляться с нестационарными задачами и разреженными градиентами. Это достигается за счет использования **экспоненциального скользящего среднего градиентов ==и их квадратов==**.
#### Основные идеи:

1. **Адаптивный шаг обучения:** Как и в RMSprop, Adam адаптирует шаг обучения для каждого параметра на основе истории градиентов.
2. **Момент первого и второго порядка:** Adam использует **==два экспоненциальных скользящих средних==**: одно для градиентов (первый момент), другое для квадратов градиентов (второй момент). Это позволяет корректировать направление и величину шага обучения более эффективно.
3. **Коррекция смещения:** На ранних этапах обучения оценки моментов могут быть смещены (так как они начинают с нулей). Adam вводит коррекцию смещения для обоих моментов, чтобы учесть это смещение.

![[Pasted image 20240819190127.png]]
![[Pasted image 20240819154243.png]]

## Как работает скользящее среднее квадратов градиентов?

**Скользящее среднее** — это метод вычисления среднего значения, который придает большее значение недавним данным, уменьшая вклад более старых данных. В контексте оптимизации это означает, что мы учитываем не только текущий градиент, но и предыдущие градиенты, с весами, уменьшающимися экспоненциально.
### Интуиция и объяснение:

1. **Зачем берется скользящее среднее?** Скользящее среднее позволяет учитывать историю градиентов, сглаживая их изменения. Это помогает избежать резких колебаний шага обучения, которые могут возникнуть при учете только текущего градиента. Градиенты на разных шагах обучения могут быть сильно различными из-за шума или особенностей ландшафта функции потерь, и скользящее среднее сглаживает эти колебания.
    
2. **Почему используется квадрат градиента?** Квадрат градиента позволяет учитывать не только направление, но и величину изменения параметров. Использование квадратов делает вклад градиентов больше в тех точках, где градиент велик, что помогает уменьшить шаг обучения на крутых участках ландшафта функции потерь. Это важно для стабильного обучения, так как слишком большой шаг на крутых участках может привести к пропуску оптимума и застреванию в колебаниях.
    
3. **Эффект затухания:** Коэффициент затухания β\betaβ определяет, насколько быстро забываются старые градиенты. Если β\betaβ близок к 1 (например, 0.9), вклад старых градиентов затухает медленно, и текущее среднее будет сильно сглажено. Это полезно, если мы хотим, чтобы оптимизатор реагировал медленно на резкие изменения градиента. Однако если β\betaβ слишком велико, адаптивность может снизиться, а если слишком мало, оптимизатор будет реагировать слишком остро на текущие значения градиента.
    
4. **Обновление параметров:** Скользящее среднее квадратов градиентов используется для масштабирования шага обучения для каждого параметра. **==Параметры, которые имеют большие градиенты (и, соответственно, большие средние квадраты градиентов), обновляются меньшими шагами, чтобы избежать резких изменений. Это помогает стабилизировать обучение и избежать взрыва градиентов.==**
### Пример

Представьте, что у вас есть функция потерь с узким глубоким минимумом. В этом случае градиенты могут быть большими за пределами минимума и маленькими внутри. Если использовать постоянный шаг обучения, модель может не успевать точно настраивать параметры в узком минимуме, так как шаги будут слишком большими. Скользящее среднее квадратов градиентов позволяет уменьшить шаг обучения, когда градиенты велики, и увеличить его, когда они малы, что помогает лучше исследовать пространство параметров и найти минимум.