#Машинное_обучение 

**Underfitting (недообучение)** - происходит, когда модель слишком проста и не способна уловить важные зависимости в данных, в результате чего модель плохо работает как на обучающих данных, так и на тестовых.
## Underfitting

![[Pasted image 20240820102709.png]]

**Annealibg** - метод динамической регулировки `lr`
- каждые `x` эпох умножать на `lambda`
- умножение `lambda` на плато - для области низкого изменения функции

При меньшем `lr` модель может обратить внимание на оптимизацию лосса более пристально и тонко

- `torch.optim.lr_sheduler`
## Методы регуляризации для борьбы с underfitting:

1. **Добавление полиномиальных признаков:**
    - Если модель слишком проста (например, линейная модель), можно попробовать добавить полиномиальные признаки, чтобы модель могла уловить нелинейные зависимости.
2. **Увеличение сложности модели:**
    - Применение более сложных моделей, таких как добавление слоев в нейронную сеть, использование более сложных архитектур (например, глубоких деревьев решений), позволяет модели лучше захватывать сложные зависимости в данных.
3. **Снижение силы регуляризации:**
    - Если регуляризация чрезмерно сильна (слишком большие значения коэффициентов регуляризации), модель может быть слишком жесткой. Ослабление регуляризации (например, уменьшение коэффициентов L1 или L2 регуляризации) может помочь модели лучше подстроиться под данные.
4. **Увеличение количества признаков:**
    - Добавление новых информативных признаков в модель, которые могут помочь ей лучше понимать зависимости в данных.
5. **Улучшение качества данных:**
    - Улучшение качества обучающих данных путем очистки от шума, добавления более информативных признаков, может помочь модели лучше справляться с задачей.