#Параллелизм 

Типичная структура программы **CUDA** состоит из пяти основных этапов:

1.    Выделение памяти **GPU**.
2.    Копирование данных из памяти **CPU** в память **GPU**.
3.    Вызов ядра **CUDA** для выполнения программно-специфических вычислений.
4.    Копирование данных обратно из памяти **GPU** в память **CPU**.
5.    Очищение памяти **GPU**.

Модель программирования **CUDA** позволяет выполнять приложения с использованием гетерогенных вычислительных систем путем простого аннотирования кода с небольшим набором расширений к языку программирования С.

Гетерогенная среда состоит из **CPU**, дополненных **GPU**, при этом **каждый из них имеет свою собственную память,** разделенную шиной **PCI-Express**. 

**Поэтому следует отметить следующие различия:**
- **Хост**: центральный процессор и его память (память хоста)
- **Устройство**: графический процессор и его память (память устройства)

Начиная с **CUDA** **6**, NVIDIA представила усовершенствованную модель программирования под названием **Unified Memory**, которая устраняет разрыв между памятью хоста и памятью устройства. Это улучшение позволяет получить доступ к памяти **CPU** и **GPU** с помощью одного указателя, в то время как система автоматически переносит данные между хостом и устройством. На текущий момент, важно научиться распределять память хоста и устройства и однозначно копировать общие между **CPU** и **GPU** данные. Этот управляемый программистами контроль памяти и данных дает возможность оптимизировать приложение и максимизировать эксплуатацию оборудования.

Ключевым компонентом модели программирования **CUDA** **является ядро **

- # Ядро - код, который выполняется на устройстве (GPU).

Как разработчик, Вы можете представить ядро в виде последовательной программы. **CUDA незаметно управляет планированием написанных программистом ядер в потоках GPU. С хоста Вы определяете, как ваш алгоритм отображается на устройстве, основываясь на данных приложения и возможностях устройства GPU.** Цель заключается в том, чтобы позволить вам сосредоточиться на логике вашего алгоритма простым способом (путем написания последовательного кода) и не погрязнуть в деталях создания и управления тысячами потоков **GPU**.

Для большинства операций хост может работать независимо от устройства. После запуска ядра управление сразу же возвращается на **хост**, освобождая **CPU** для выполнения дополнительных задач, которые дополняются параллельным кодом данных, запущенным на устройстве.

- #### Обычная программа CUDA состоит из последовательного кода, дополненного параллельным кодом.

Код хоста написан на **ANSI C**, а код устройства - на **CUDA C**. 

Весь код можно поместить в один исходный файл, либо использовать несколько исходных файлов для сборки приложения или библиотек. **Компилятор NVIDIA C (nvcc) генерирует исполняемый код как для хоста, так и для устройства.**

- #### Параллельная часть (ядра) выполняется на GPU, а обычная часть - на CPU. 

Обычно процесс обработки данных в программе **CUDA** проходит по такой схеме:
1. Копирование данных из памяти CPU в память GPU.****
2. Вызов ядер для работы с данными, хранящимися в памяти GPU.
3. Копирование данных обратно из памяти GPU в память CPU.

| Спецификатор | откуда вызывается | откуда может вызываться |
|---|---|---|
| **\_\_device\_\_** | device | device |
| **\_\_global\_\_** | device | host |
| **\_\_host\_\_** | host | host |
## Пример 1

![[Pasted image 20231102152912.png]]

1. В начале  прописываем спецификатор **\_\_global\_\_**, внутри которого 3 аргумента — 2 числа и результат их сложения.
2. В теле этой функции прописываем саму арифметическую операцию **\*c = \*a + \*b**.
3. **В главной функции переменные разделяются на две части: на хосте и на устройстве** (названия переменных помечены соответственно), кроме этого указывается переменная, отвечающая за размер данных — **int size**. 
4. Далее выделяем память на GPU с помощью команды **cudaMalloc**, в аргументах прописаны указатели памяти типа **void** поадресно (dev_a, dev_b, dev_c) и выделяемый размер памяти для каждой переменной.
5. Прописываем значения переменных _a_ и _b_ (например 5 и 10). 
6. Копируем значения на **GPU** - **cudaMemcpy**
7. Запускаем ядро **GPU** с помощью команды **add<<< 1, 1 >>>(dev_a, dev_b, dev_c);.** 
8. Копируем результат переменной _c_ обратно на **CPU** и освобождаем память.

## Пример 2

