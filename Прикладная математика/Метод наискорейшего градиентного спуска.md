#Алгоритмы #Итерационные_методы #Оптимизация_управления 
[[Математическая функция]]
[[Градиент]]

- ## Алгоритм

![[Pasted image 20231120133600.png]]

Алгоритм минимизации с использованием градиента построен на том, что если **градиент** это вектор направление наискорейшего роста, то двигаясь против градиента мы будем двигаться в направлении убывания и придем к минимуму функции. 

Если мы **перепрыгнем** через точку минимума, тогда поскольку градиент уже будет указывать в **противоположную** сторону, то мы пойдем **обратно** к минимуму.

По мере приближения градиент уменьшается, а значит при **b = const** будет сильное замедление в **окрестности решения**. **Для этого на каждом шаге вычисляется оптимальное значение шагового множителя** `b` путем одномерной минимизации функции относительно `b`.
### **Одномерная функция J(u)**

В точке минимума **касательная будет параллельна** оси `OX` и угол будет **равен нулю**, соответственно вектор из производных, характеризующих направление и скорость роста функции будут нулевыми т.к. **tg 0 = 0**. 

1. Задаем начальную точку `ua = uk`, а точку `ub = ua + delta`  
2. Пока `gradJ(u) > eps`:
	1. Вычисляем градиент в точке `uk`
	2. **На каждом шаге вычисляем оптимальный шаговый множитель** `b`:
		1. Пока `abs(u2 - u1) > eps:` проводим одномерную минимизацию `J(b)=J(uk - b * gradJ(uk))` методом **дихотомии** или **золотого сечения**, что даст лучшую производительность за счет меньших вычислений значения `J` 
		2. Если `J(uk - u2 * grad_J_uk) > J(uk - u1 * grad_J_uk):` то `ub = u2`, иначе `ua = u1`
		3. При наступлении условия останова (достижения заданной точности), возвращаем `(ua + ub) / 2`
	3. Вычисляем `uk`
			![[Pasted image 20231120134727.png]]

- ## Особенности


![[Pasted image 20231120140010.png]]

Для удобства представим это в двумерной ситуации. **Плоскостями, параллельными**` u1u2` как бы разрезаем **параболоид вращения** и получаем **линии постоянного уровня**. **Градиент будет перпендикулярен линиям уровня, если все компоненты J вносят одинаковый вклад**. 

![[Pasted image 20231120141814.png]]

В таком случае **критерий качества**, описываемый функцией` J` имеет разную чувствительность к компонентам.
Линии уровня не являются окружностями, а эллипсами с различными искривлениями.

Градиент также перпендикулярен линиям постоянного уровня, **но антиградиент не ведет к минимуму**, а ведет на **линию дна овражной функции**. Тогда находим минимум вдоль направления антиградиента `gradJ[k]`, находим уже градиент следующего шага `gradJ[k+1]`, где он перпендикулярен направлению предыдущего шага. Около дна мы будем идти перпендикулярными шагами, что **уменьшает сходимость** в этой области.

- ## Преимущества

- ## Недостатки

- # Ухудшается сходимость при разной чувствительности критерия к компонентам (в случае n-мерной функции)