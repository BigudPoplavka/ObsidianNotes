#Статистика 

**Стандартное отклонение (Standard Deviation)** — это статистическая мера, которая показывает, насколько данные в выборке отклоняются от среднего значения. Она представляет собой квадратный корень из дисперсии и является одним из основных показателей разброса данных.

![[Pasted image 20240826154802.png]]

Здесь:

- `n` — количество наблюдений.
- `x_i`​ — каждое отдельное значение.
- `x_` — среднее значение выборки.

### Применение стандартного отклонения в нейронных сетях

В нейронных сетях стандартное отклонение используется в нескольких контекстах:

1. **Нормализация данных:**
    
    - Перед подачей данных в нейронную сеть часто выполняется **нормализация или стандартизация**. Это помогает ускорить обучение и улучшить сходимость модели. В процессе стандартизации данные приводятся к нулевому среднему и единичному стандартному отклонению:
    - ![[Pasted image 20240826155602.png]]
    - где `μ` — среднее значение признака, а `σ` — его стандартное отклонение. Это делает данные более однородными и помогает сети лучше обучаться.
- **Инициализация весов:**
    
    - При инициализации весов нейронной сети правильный выбор начальных значений весов важен для успешного обучения. Один из распространенных методов инициализации — **Xavier (Glorot) инициализация**, где веса инициализируются с учетом дисперсии (которая связана со стандартным отклонением). Это позволяет избежать проблемы затухающих или взрывающихся градиентов, что особенно важно для глубоких сетей.

- **Оценка неопределенности:**
	    
    - В некоторых случаях, например, в байесовских нейронных сетях, стандартное отклонение используется для оценки неопределенности предсказаний. Вместо точечного предсказания нейронная сеть может предсказывать распределение (среднее и стандартное отклонение), что позволяет лучше учитывать неопределенность в данных.

- **Анализ градиентов и весов:**
    
    - Во время обучения нейронной сети полезно отслеживать стандартное отклонение градиентов и весов. Слишком малое или слишком большое стандартное отклонение может указывать на проблемы с обучением, такие как взрывающиеся или затухающие градиенты.
    
- **Batch Normalization:**
    
    - В процессе обучения нейронных сетей широко используется метод **Batch Normalization**, который нормализует входные данные для каждого слоя с использованием среднего и стандартного отклонения для мини-батча данных. Это стабилизирует и ускоряет процесс обучения.